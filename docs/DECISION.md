# Local Brain: Architecture Decision Record

**Date:** December 8, 2025  
**Status:** Approved  
**Context:** Claude Code skill delegation to local Ollama models

---

## Executive Summary

After multi-model research analysis, the decision is to **keep the current implementation** with incremental improvements, while evaluating Smolagents as a future option.

---

## The Actual Use Case

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     delegates      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     calls      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Code â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Local Brain â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Ollama  â”‚
â”‚   (Cloud)   â”‚                    â”‚   (Local)   â”‚                â”‚ (Local) â”‚
â”‚             â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚             â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     returns        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    responds    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    results                           with tools
```

**Key Insight:** Local Brain is NOT a standalone CLI competing with Aider. It's a **delegation target** for Claude Code skills, providing local model access with codebase tools.

---

## Decision: Keep Current Implementation

### Why NOT Aider
- Aider is **interactive** (designed for humans in terminals)
- No programmatic API for delegation
- Expects user input/confirmation loops
- Cannot be called as a subprocess with structured results

### Why NOT Frameworks (LangChain, LlamaIndex, etc.)
- **Overkill**: 50+ dependencies vs 2 (ollama, click)
- **Abstraction overhead**: We talk directly to Ollama
- **Maintenance burden**: Frameworks evolve, APIs break
- Current ~450 lines of focused code is the right size

### Why Keep Current
- âœ… **Works** for the delegation use case
- âœ… **Minimal** dependencies
- âœ… **Focused** on read-only codebase exploration
- âœ… **Simple** to maintain and extend
- âœ… **Native** Ollama integration

---

## Roadmap

### Phase 1: Now - Improve Current Implementation

| Task | Priority |
|------|----------|
| Add path jailing (restrict to project root) | High |
| Add structured JSON output option | High |
| Add tests for allowlist/denylist behavior | Medium |
| Add retry logic for Ollama calls | Low |

### Phase 2: Next - Evaluate Smolagents

**What:** Code-as-tool pattern where model writes Python instead of calling fixed tools.

**Why consider:**
- Eliminates tool maintenance entirely
- Sandboxed execution (better than regex allowlists)
- Model writes `import os; os.listdir('.')` instead of calling `list_directory` tool

**Experiment:**
1. Create `feature/smolagents` branch
2. Test with Qwen-Coder via Ollama
3. Validate code generation quality
4. If works: Replace `local_brain/tools/`
5. If doesn't: Keep current approach

### Phase 3: Future - Consider MCP Bridge

**What:** Model Context Protocol (MCP) is an emerging standard for LLM tooling.

**Opportunity:** Local Brain could become the **Ollama â†” MCP bridge**:
- Receive tool schemas from MCP servers
- Translate to Ollama's tool format
- Execute via Ollama
- Return results via MCP protocol

**When to pursue:**
- MCP adoption increases
- Community needs Ollamaâ†”MCP connectivity
- High effort, high risk, high potential reward

---

## Alternatives Considered

| Alternative | Verdict | Reason |
|-------------|---------|--------|
| **Aider** | âŒ Rejected | Interactive, not programmable |
| **LangChain** | âŒ Rejected | Too heavy (50+ deps), overkill |
| **LlamaIndex** | âŒ Rejected | RAG-focused, not tool-focused |
| **AutoGen** | âŒ Rejected | Multi-agent, overkill |
| **CrewAI** | âŒ Rejected | Multi-agent, overkill |
| [**Smolagents**](https://github.com/huggingface/smolagents) | ğŸ”„ Evaluate | Promising code-as-tool pattern |
| **MCP Bridge** | ğŸ”® Future | If standard gains traction |

---

## Current Architecture (Approved)

```
Claude Code Skill
    â”‚
    â””â”€â”€â–º local-brain CLI
            â”‚
            â”œâ”€â”€â–º agent.py (tool loop)
            â”‚       â”‚
            â”‚       â””â”€â”€â–º ollama.chat(tools=[...])
            â”‚
            â””â”€â”€â–º tools/
                    â”œâ”€â”€ file_tools.py   (read_file, list_directory, file_info)
                    â”œâ”€â”€ git_tools.py    (git_diff, git_status, git_log, git_changed_files)
                    â””â”€â”€ shell_tools.py  (run_command with allowlist)
```

### Strengths
- Direct `ollama-python` SDK usage
- ~450 lines of focused code
- 2 dependencies only
- Read-only security posture

### Known Limitations
- Regex-based command allowlist (fragile)
- No path jailing
- No structured output format
- Basic error handling

---

## Research Archive

The following documents were synthesized into this decision:

| Document | Model | Stance |
|----------|-------|--------|
| `RESEARCH-claude-4.5-opus-high.md` | Claude 4.5 Opus | Deprecate or pivot |
| `RESEARCH-composer-1.md` | Composer | Keep as-is |
| `RESEARCH-gemini-3-pro.md` | Gemini 3 Pro | Use Smolagents |
| `RESEARCH-gpt-5.1-codex-max.md` | GPT-5.1 Codex Max | Redundant but valuable |

All models agreed:
- Current code is clean and well-written
- The LLM tooling space has mature alternatives
- Security implementation could be improved

The key disagreement was resolved by recognizing the **actual use case** (delegation, not standalone CLI).

---

## Conclusion

**Keep Local Brain simple and focused.** It serves a specific purpose (Claude Code â†’ Ollama delegation) that mature alternatives like Aider don't address. Improve incrementally, evaluate Smolagents when ready, and watch MCP adoption for future opportunities.

---

*Decision approved: December 8, 2025*

